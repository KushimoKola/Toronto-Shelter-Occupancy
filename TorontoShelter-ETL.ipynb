{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# Toronto Open Data is stored in a CKAN instance. Its APIs are documented here:\n",
    "# https://docs.ckan.org/en/latest/api/\n",
    "\n",
    "# Htting Toronto's open data  API\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "\n",
    "# Datasets are called \"packages\". Each package can contain many \"resources\"\n",
    "# To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "url = base_url + \"/api/3/action/package_show\"\n",
    "params = {\"id\": \"daily-shelter-overnight-service-occupancy-capacity\"}\n",
    "package = requests.get(url, params=params).json()\n",
    "\n",
    "# I created a directory to store the CSV files\n",
    "output_dir = os.getcwd()  # This is to get my current working directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Since Data were in different files, I had to create a single directory to store all the files\n",
    "output_file_path = os.path.join(output_dir, \"combined_results.csv\")\n",
    "\n",
    "# The logic here is to load existing idempotent keys from the previously processed CSV file (if it exists)\n",
    "existing_idempotent_keys = set()\n",
    "if os.path.exists(output_file_path):\n",
    "    with open(output_file_path, \"r\", newline=\"\", encoding=\"utf-8\") as existing_file:\n",
    "        existing_csv_reader = csv.reader(existing_file)\n",
    "        next(existing_csv_reader)  # Skip header\n",
    "        for row in existing_csv_reader:\n",
    "            existing_idempotent_keys.add(row[0])  # Assuming idempotent_key is the first column\n",
    "\n",
    "# This to determine count of total data and new data\n",
    "total_data_count = 0\n",
    "new_data_count = 0\n",
    "\n",
    "with open(output_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "    csv_writer = csv.writer(output_file)\n",
    "    current_header = None\n",
    "\n",
    "    # To get resource data:\n",
    "    for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "        # for datastore_active resources:\n",
    "        if resource[\"datastore_active\"]:\n",
    "\n",
    "            # To get all records in CSV format:\n",
    "            url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
    "            resource_dump_data = requests.get(url).text\n",
    "\n",
    "            # Some columns were being dramatic, so, I had to clean each field by\n",
    "            # removing commas and quotes within the fields\n",
    "            cleaned_rows = []\n",
    "            for row in resource_dump_data.split(\"\\n\"):\n",
    "                cleaned_row = re.sub(r'\"(.*?)\"', lambda match: match.group(1).replace(\",\", \"\"), row)\n",
    "                cleaned_rows.append(cleaned_row)\n",
    "\n",
    "            # When data was appended, it appended headers also, causing a bit of problem\n",
    "            #Since all column names are the same, I had to pick 1st index of the columns\n",
    "            if idx == 0:\n",
    "                current_header = cleaned_rows[0]\n",
    "                csv_writer.writerow([\"IDEMPOTENT_KEY\"] + current_header.split(\",\"))  # This added the indempotent_key to the header\n",
    "\n",
    "            # Write the cleaned data to the CSV file, skip header for appended data\n",
    "            for cleaned_row in cleaned_rows[1:]:\n",
    "                if cleaned_row.strip():\n",
    "                    data_fields = cleaned_row.split(\",\")\n",
    "\n",
    "                    # This logic give a uniform OCCUPANCY_DATE to yyyy-mm-dd format\n",
    "                    date_index = current_header.split(\",\").index(\"OCCUPANCY_DATE\")\n",
    "                    original_date = data_fields[date_index]\n",
    "                    \n",
    "                    # Assumptions of different date formats - I handle the different date formats here\n",
    "                    formatted_date = None\n",
    "                    try:\n",
    "                        formatted_date = datetime.strptime(original_date, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "                    except ValueError:\n",
    "                        try:\n",
    "                            formatted_date = datetime.strptime(original_date, \"%y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "                        except ValueError:\n",
    "                            pass  # If I have missed any other date formats, so it doesn't throw an error\n",
    "\n",
    "                    if formatted_date:\n",
    "                        data_fields[date_index] = formatted_date\n",
    "\n",
    "                    # Create idempotent key by combining _id and OCCUPANCY_DATE\n",
    "                    id_index = current_header.split(\",\").index(\"_id\")\n",
    "                    id_value = data_fields[id_index]\n",
    "                    idempotent_key = f\"{id_value}_{formatted_date}\"\n",
    "\n",
    "                    # Hash the idempotent key for uniqueness\n",
    "                    hashed_key = hashlib.sha3_224(idempotent_key.encode()).hexdigest()[:20]\n",
    "                    \n",
    "                    if hashed_key not in existing_idempotent_keys:\n",
    "                        csv_writer.writerow([hashed_key] + data_fields)\n",
    "                        new_data_count += 1\n",
    "                        existing_idempotent_keys.add(hashed_key)\n",
    "\n",
    "                    total_data_count += 1\n",
    "\n",
    "print(\"Data processing complete.\")\n",
    "print(\"Total data count:\", total_data_count)\n",
    "print(\"New data count:\", new_data_count)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
