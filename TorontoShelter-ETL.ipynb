{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n",
      "Total data count: 132507\n",
      "New data count: 254\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# Toronto Open Data is stored in a CKAN instance. Its APIs are documented here:\n",
    "# https://docs.ckan.org/en/latest/api/\n",
    "\n",
    "# Hitting Toronto's open data API\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "\n",
    "# Datasets are called \"packages\". Each package can contain many \"resources\"\n",
    "# To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "url = base_url + \"/api/3/action/package_show\"\n",
    "params = {\"id\": \"daily-shelter-overnight-service-occupancy-capacity\"}\n",
    "package = requests.get(url, params=params).json()\n",
    "\n",
    "# I created a directory to store the CSV files\n",
    "output_dir = os.getcwd()  # This is to get my current working directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Since Data were in different files, I had to create a single directory to store all the files\n",
    "output_file_path = os.path.join(output_dir, \"Toronto Shelter Occupancy.csv\")\n",
    "\n",
    "# This to determine count of total data and new data\n",
    "total_data_count = 0\n",
    "new_data_count = 0\n",
    "\n",
    "# Logic to determine if header has been appended\n",
    "header_appended = False\n",
    "current_header = None\n",
    "\n",
    "# Function to clean data\n",
    "def clean_data(row):\n",
    "    cleaned_row = re.sub(r'\"(.*?)\"', lambda match: match.group(1).replace(\",\", \"\"), row)\n",
    "    return cleaned_row\n",
    "\n",
    "# Create a set to store existing idempotent keys\n",
    "existing_idempotent_keys = set()\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_file_path):\n",
    "    # Read existing idempotent keys from the previously processed CSV file\n",
    "    with open(output_file_path, \"r\", newline=\"\", encoding=\"utf-8\") as existing_file:\n",
    "        existing_csv_reader = csv.reader(existing_file)\n",
    "        first_row = next(existing_csv_reader, None)  # Attempt to read the first row (header)\n",
    "        if first_row:\n",
    "            header_appended = True\n",
    "            current_header = first_row[1:]  # Skip the first column (IDEMPOTENT_KEY)\n",
    "            for row in existing_csv_reader:\n",
    "                existing_idempotent_keys.add(row[0])  # Assuming idempotent_key is the first column\n",
    "\n",
    "# Open the output file for writing, with proper newline handling\n",
    "with open(output_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "    csv_writer = csv.writer(output_file)\n",
    "\n",
    "    # To get resource data:\n",
    "    for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "        # for datastore_active resources:\n",
    "        if resource[\"datastore_active\"]:\n",
    "            # To get all records in CSV format:\n",
    "            url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
    "            resource_dump_data = requests.get(url).text\n",
    "\n",
    "            # Split data into rows\n",
    "            rows = resource_dump_data.split(\"\\n\")\n",
    "\n",
    "            # Process each row\n",
    "            for row in rows:\n",
    "                if row.strip():\n",
    "                    # This is the first run or header has not been appended\n",
    "                    if not header_appended:\n",
    "                        current_header = clean_data(row).split(\",\")\n",
    "                        csv_writer.writerow([\"IDEMPOTENT_KEY\"] + current_header)  # This added the idempotent_key to the header\n",
    "                        header_appended = True\n",
    "                    else:\n",
    "                        # This is not the first run, process and append new data\n",
    "                        data_fields = clean_data(row).split(\",\")\n",
    "                        \n",
    "                        # This logic gives a uniform OCCUPANCY_DATE to yyyy-mm-dd format\n",
    "                        date_index = current_header.index(\"OCCUPANCY_DATE\")\n",
    "                        original_date = data_fields[date_index]\n",
    "\n",
    "                        # Assumptions of different date formats - I handle the different date formats here\n",
    "                        formatted_date = None\n",
    "                        try:\n",
    "                            formatted_date = datetime.strptime(original_date, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                formatted_date = datetime.strptime(original_date, \"%y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "                            except ValueError:\n",
    "                                pass  # If I have missed any other date formats, so it doesn't throw an error\n",
    "\n",
    "                        if formatted_date:\n",
    "                            data_fields[date_index] = formatted_date\n",
    "\n",
    "                        # Create an idempotent key by combining _id and OCCUPANCY_DATE\n",
    "                        id_index = current_header.index(\"_id\")\n",
    "                        id_value = data_fields[id_index]\n",
    "                        \n",
    "                        # Exclude header rows from idempotent key creation\n",
    "                        if id_value != \"_id\":\n",
    "                            idempotent_key = f\"{id_value}_{formatted_date}\"\n",
    "\n",
    "                            # Hash the idempotent key for uniqueness\n",
    "                            hashed_key = hashlib.sha3_224(idempotent_key.encode()).hexdigest()[:20]\n",
    "\n",
    "                            # Check if the hashed_key already exists\n",
    "                            if hashed_key not in existing_idempotent_keys:\n",
    "                                csv_writer.writerow([hashed_key] + data_fields)\n",
    "                                new_data_count += 1\n",
    "\n",
    "                    total_data_count += 1\n",
    "\n",
    "print(\"Data processing complete.\")\n",
    "print(\"Total data count:\", total_data_count)\n",
    "print(\"New data count:\", new_data_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
