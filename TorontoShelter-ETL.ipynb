{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.6/62.6 kB ? eta 0:00:00\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.8/163.8 kB ? eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "   ---------------------------------------- 0.0/100.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 100.4/100.4 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 66.8/66.8 kB ? eta 0:00:00\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.1/121.1 kB ? eta 0:00:00\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 idna-3.7 requests-2.31.0 urllib3-2.2.1\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.0/61.0 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kolawole\\anaconda3\\envs\\windows\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kolawole\\anaconda3\\envs\\windows\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.7/11.5 MB 36.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.9/11.5 MB 75.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.5 MB 70.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.5 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 65.6 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 4.8/15.5 MB 101.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.3/15.5 MB 87.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.3/15.5 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.1/15.5 MB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 65.5 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   ---------------------------------------- 0.0/505.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 505.5/505.5 kB ? eta 0:00:00\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 345.4/345.4 kB 22.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "#Install Required Packages\n",
    "!pip install requests\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n",
      "Total data count: 165621\n",
      "New data count: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# Toronto Open Data is stored in a CKAN instance. Its APIs are documented here:\n",
    "# https://docs.ckan.org/en/latest/api/\n",
    "\n",
    "# Hitting Toronto's open data API\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "\n",
    "# Datasets are called \"packages\". Each package can contain many \"resources\"\n",
    "# To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "url = base_url + \"/api/3/action/package_show\"\n",
    "params = {\"id\": \"daily-shelter-overnight-service-occupancy-capacity\"}\n",
    "package = requests.get(url, params=params).json()\n",
    "\n",
    "# I created a directory to store the CSV files\n",
    "output_dir = os.getcwd()  # This is to get my current working directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Since Data were in different files, I had to create a single directory to store all the files\n",
    "output_file_path = os.path.join(output_dir, \"toronto_shelter_occupancy.csv\")\n",
    "\n",
    "# This to determine count of total data and new data\n",
    "total_data_count = 0\n",
    "new_data_count = 0\n",
    "\n",
    "# Logic to determine if header has been appended\n",
    "header_appended = False\n",
    "current_header = None\n",
    "\n",
    "# Function to clean data\n",
    "def clean_data(row):\n",
    "    cleaned_row = re.sub(r'\"(.*?)\"', lambda match: match.group(1).replace(\",\", \"\"), row)\n",
    "    return cleaned_row\n",
    "\n",
    "# Create a set to store existing idempotent keys\n",
    "existing_idempotent_keys = set()\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_file_path):\n",
    "    # Read existing idempotent keys from the previously processed CSV file\n",
    "    with open(output_file_path, \"r\", newline=\"\", encoding=\"utf-8\") as existing_file:\n",
    "        existing_csv_reader = csv.reader(existing_file)\n",
    "        first_row = next(existing_csv_reader, None)  # Attempt to read the first row (header)\n",
    "        if first_row:\n",
    "            header_appended = True\n",
    "            current_header = first_row[1:]  # Skip the first column (IDEMPOTENT_KEY)\n",
    "            for row in existing_csv_reader:\n",
    "                existing_idempotent_keys.add(row[0])  # Assuming idempotent_key is the first column\n",
    "\n",
    "# Open the output file for writing, with proper newline handling\n",
    "with open(output_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "    csv_writer = csv.writer(output_file)\n",
    "\n",
    "    # To get resource data:\n",
    "    for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "        # for datastore_active resources:\n",
    "        if resource[\"datastore_active\"]:\n",
    "            # To get all records in CSV format:\n",
    "            url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
    "            resource_dump_data = requests.get(url).text\n",
    "\n",
    "            # Split data into rows\n",
    "            rows = resource_dump_data.split(\"\\n\")\n",
    "\n",
    "            # Process each row\n",
    "            for row in rows:\n",
    "                if row.strip():\n",
    "                    # This is the first run or header has not been appended\n",
    "                    if not header_appended:\n",
    "                        current_header = clean_data(row).split(\",\")\n",
    "                        csv_writer.writerow([\"IDEMPOTENT_KEY\"] + current_header)  # This added the idempotent_key to the header\n",
    "                        header_appended = True\n",
    "                    else:\n",
    "                        # This is not the first run, process and append new data\n",
    "                        data_fields = clean_data(row).split(\",\")\n",
    "                        \n",
    "                        # This logic gives a uniform OCCUPANCY_DATE to yyyy-mm-dd format\n",
    "                        date_index = current_header.index(\"OCCUPANCY_DATE\")\n",
    "                        original_date = data_fields[date_index]\n",
    "\n",
    "                        # Assumptions of different date formats - I handle the different date formats here\n",
    "                        formatted_date = None\n",
    "                        try:\n",
    "                            formatted_date = datetime.strptime(original_date, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                formatted_date = datetime.strptime(original_date, \"%y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "                            except ValueError:\n",
    "                                pass  # If I have missed any other date formats, so it doesn't throw an error\n",
    "\n",
    "                        if formatted_date:\n",
    "                            data_fields[date_index] = formatted_date\n",
    "\n",
    "                        # Create an idempotent key by combining _id and OCCUPANCY_DATE\n",
    "                        id_index = current_header.index(\"_id\")\n",
    "                        id_value = data_fields[id_index]\n",
    "                        \n",
    "                        # Exclude header rows from idempotent key creation\n",
    "                        if id_value != \"_id\":\n",
    "                            idempotent_key = f\"{id_value}_{formatted_date}\"\n",
    "\n",
    "                            # Hash the idempotent key for uniqueness\n",
    "                            hashed_key = hashlib.sha3_224(idempotent_key.encode()).hexdigest()[:20]\n",
    "\n",
    "                            # Check if the hashed_key already exists\n",
    "                            if hashed_key not in existing_idempotent_keys:\n",
    "                                csv_writer.writerow([hashed_key] + data_fields)\n",
    "                                new_data_count += 1\n",
    "\n",
    "                    total_data_count += 1\n",
    "\n",
    "print(\"Data processing complete.\")\n",
    "print(\"Total data count:\", total_data_count)\n",
    "print(\"New data count:\", new_data_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
