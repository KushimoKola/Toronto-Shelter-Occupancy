{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create venv for project\n",
    "python3 -m venv .tsheltervenv  # only once\n",
    "\n",
    "#Activate venv\n",
    "source .tsheltervenv/bin/activate\n",
    "\n",
    "#Freeze requirement\n",
    "pip3 freeze > requirements.txt  # only once\n",
    "\n",
    "#Install requirements\n",
    "pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n",
      "Total data count: 198559\n",
      "New data count: 11451\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------FETCH DATA FROM API---------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Toronto Open Data is stored in a CKAN instance. Its APIs are documented here:\n",
    "# https://docs.ckan.org/en/latest/api/\n",
    "\n",
    "# Hitting Toronto's open data API\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "\n",
    "# Datasets are called \"packages\". Each package can contain many \"resources\"\n",
    "# To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "url = base_url + \"/api/3/action/package_show\"\n",
    "params = {\"id\": \"daily-shelter-overnight-service-occupancy-capacity\"}\n",
    "package = requests.get(url, params=params).json()\n",
    "\n",
    "# I created a directory to store the CSV files\n",
    "output_dir = os.getcwd()  # This is to get my current working directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Since Data were in different files, I had to create a single directory to store all the files\n",
    "output_file_path = os.path.join(output_dir, \"toronto_shelter_occupancy.csv\")\n",
    "\n",
    "# This to determine count of total data and new data\n",
    "total_data_count = 0\n",
    "new_data_count = 0\n",
    "\n",
    "# Logic to determine if header has been appended\n",
    "header_appended = False\n",
    "current_header = None\n",
    "\n",
    "\n",
    "\"\"\"# Function to clean data\n",
    "def clean_data(row):\n",
    "    cleaned_row = re.sub(r'\"(.*?)\"', lambda match: match.group(1).replace(\",\", \"\"), row)\n",
    "    return cleaned_row\"\"\"\n",
    "    \n",
    "# Function to clean data\n",
    "def clean_data(row):\n",
    "    # Remove quotes and any newline characters within the field\n",
    "    cleaned_row = re.sub(r'\"(.*?)\"', lambda match: match.group(1).replace(\",\", \"\"), row).strip()\n",
    "    return cleaned_row\n",
    "\n",
    "# Create a set to store existing idempotent keys\n",
    "existing_idempotent_keys = set()\n",
    "\n",
    "# Check if the output file already exists\n",
    "if os.path.exists(output_file_path):\n",
    "    # Read existing idempotent keys from the previously processed CSV file\n",
    "    with open(output_file_path, \"r\", newline=\"\", encoding=\"utf-8\") as existing_file:\n",
    "        existing_csv_reader = csv.reader(existing_file)\n",
    "        first_row = next(existing_csv_reader, None)  # Attempt to read the first row (header)\n",
    "        if first_row:\n",
    "            header_appended = True\n",
    "            current_header = first_row[1:]  # Skip the first column (IDEMPOTENT_KEY)\n",
    "            for row in existing_csv_reader:\n",
    "                existing_idempotent_keys.add(row[0])  # Assuming idempotent_key is the first column\n",
    "\n",
    "# Open the output file for writing, with proper newline handling\n",
    "with open(output_file_path, \"a\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "    csv_writer = csv.writer(output_file)\n",
    "\n",
    "    # To get resource data:\n",
    "    for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "        # for datastore_active resources:\n",
    "        if resource[\"datastore_active\"]:\n",
    "            # To get all records in CSV format:\n",
    "            url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
    "            resource_dump_data = requests.get(url).text\n",
    "\n",
    "            # Split data into rows\n",
    "            rows = resource_dump_data.split(\"\\n\")\n",
    "            \n",
    "            for row in rows:\n",
    "                if row.strip():  # Check if row is not empty\n",
    "                    if not header_appended:\n",
    "                        current_header = clean_data(row).split(\",\")\n",
    "                        csv_writer.writerow([\"IDEMPOTENT_KEY\"] + current_header)  # This adds the idempotent_key to the header\n",
    "                        header_appended = True\n",
    "                    else:\n",
    "                        data_fields = clean_data(row).split(\",\")\n",
    "                        # This logic gives a uniform OCCUPANCY_DATE to yyyy-mm-dd format\n",
    "                        date_index = current_header.index(\"OCCUPANCY_DATE\")\n",
    "                        original_date = data_fields[date_index]\n",
    "                        formatted_date = None\n",
    "                        try:\n",
    "                            formatted_date = datetime.strptime(original_date, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                formatted_date = datetime.strptime(original_date, \"%y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "                            except ValueError:\n",
    "                                pass  # If I have missed any other date formats, so it doesn't throw an error\n",
    "\n",
    "                        if formatted_date:\n",
    "                            data_fields[date_index] = formatted_date\n",
    "\n",
    "                        # Create an idempotent key by combining _id and OCCUPANCY_DATE\n",
    "                        id_index = current_header.index(\"_id\")\n",
    "                        id_value = data_fields[id_index]\n",
    "                        \n",
    "                        # Exclude header rows from idempotent key creation\n",
    "                        if id_value != \"_id\":\n",
    "                            idempotent_key = f\"{id_value}_{formatted_date}\"\n",
    "\n",
    "                            # Hash the idempotent key for uniqueness\n",
    "                            hashed_key = hashlib.sha3_224(idempotent_key.encode()).hexdigest()[:20]\n",
    "\n",
    "                            # Check if the hashed_key already exists\n",
    "                            if hashed_key not in existing_idempotent_keys:\n",
    "                                csv_writer.writerow([hashed_key] + data_fields)\n",
    "                                new_data_count += 1\n",
    "\n",
    "                    total_data_count += 1\n",
    "\n",
    "print(\"Data processing complete.\")\n",
    "print(\"Total data count:\", total_data_count)\n",
    "print(\"New data count:\", new_data_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"!pip install pandas sqlalchemy openpyxl\n",
    "!pip install psycopg2-binary\n",
    "!pip install python-dotenv\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table 'toronto_shelther_occupancy' already exists. Appending new data while ignoring duplicates.\n",
      "No new data found.\n",
      "Temporary file 'temp.csv' has been deleted.\n",
      "The table 'toronto_shelther_occupancy' now has 198554 rows after the update.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------LOAD FETCHED DATA INTO DATABASE---------\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, inspect\n",
    "from sqlalchemy.dialects.postgresql import VARCHAR, TIMESTAMP\n",
    "from sqlalchemy.sql import text\n",
    "from urllib.parse import quote_plus\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "#Path to your CSV file\n",
    "file_path = 'toronto_shelter_occupancy.csv'\n",
    "\n",
    "#Read the CSV data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\"\"\"#Define a function to convert columns to datetime with the correct timezone\n",
    "def convert_to_datetime_with_timezone(df, col_name):\n",
    "    df[col_name] = pd.to_datetime(df[col_name], utc=True, errors='coerce')\n",
    "    df[col_name] = df[col_name].dt.tz_convert('US/Eastern')\n",
    "\n",
    "#Convert 'Created At' and 'Updated At' columns to datetime with EST timezone\n",
    "convert_to_datetime_with_timezone(df, 'Time')\n",
    "convert_to_datetime_with_timezone(df, 'Updated At')  # Uncomment if column exists\n",
    "\"\"\"\n",
    "#Simplify and consolidate column name formatting\n",
    "df.columns = df.columns.str.replace(' ', '_', regex=True).str.replace('[().?]', '', regex=True).str.lower()\n",
    "\n",
    "#Database connection details\n",
    "database_username = os.getenv('database_username')\n",
    "database_password = os.getenv('database_password')\n",
    "database_name = 'analytics_db'\n",
    "database_host = 'localhost'\n",
    "database_port = '5432'\n",
    "database_schema = 'toronto'\n",
    "\n",
    "database_connection_string = f'postgresql://{database_username}:{database_password}@{database_host}:{database_port}/{database_name}'\n",
    "engine = create_engine(database_connection_string)\n",
    "\n",
    "#Initialize MetaData object\n",
    "metadata = MetaData()\n",
    "\n",
    "#Define columns for the table\n",
    "columns = []\n",
    "for column_name, dtype in df.dtypes.items():\n",
    "    if \"datetime64[ns]\" in str(dtype):\n",
    "        columns.append(Column(column_name, TIMESTAMP(timezone=True)))\n",
    "    else:\n",
    "        columns.append(Column(column_name, VARCHAR))\n",
    "\n",
    "#Name of the table in PostgreSQL\n",
    "table_name = 'toronto_shelther_occupancy'\n",
    "\n",
    "#Create a table with the defined columns within the specified schema\n",
    "table = Table(table_name, metadata, *columns, schema=database_schema, extend_existing=True)\n",
    "\n",
    "#Check if table exists\n",
    "with engine.connect() as connection:\n",
    "    inspector = inspect(engine)\n",
    "    table_exists = inspector.has_table(table_name, schema=database_schema)\n",
    "\n",
    "    if not table_exists:\n",
    "        # Create table\n",
    "        metadata.create_all(engine)\n",
    "        print(f\"The table '{table_name}' has been created.\")\n",
    "        \n",
    "        # Initial insertion of data\n",
    "        df.to_sql(table_name, engine, if_exists='append', index=False, schema=database_schema)\n",
    "        print(f\"Data inserted into '{table_name}' for the first time.\")\n",
    "    else:\n",
    "        print(f\"The table '{table_name}' already exists. Appending new data while ignoring duplicates.\")\n",
    "        \n",
    "        # Dynamically retrieve the column names from the table\n",
    "        columns = inspector.get_columns(table_name, schema=database_schema)\n",
    "        idempotent_key = [col['name'] for col in columns if col['name'] == 'idempotent_key'][0]\n",
    "        \n",
    "        # Create a temporary CSV file\n",
    "        temp_csv_path = 'temp.csv'\n",
    "        df.to_csv(temp_csv_path, index=False)\n",
    "        \n",
    "        # Read existing ids from the database table\n",
    "        existing_ids_query = text(f\"SELECT {idempotent_key} FROM {database_schema}.{table_name}\")\n",
    "        existing_ids = pd.read_sql(existing_ids_query, engine)[idempotent_key].tolist()\n",
    "        \n",
    "        # Filter the temporary CSV file to exclude existing ids\n",
    "        temp_df = pd.read_csv(temp_csv_path)\n",
    "        new_data = temp_df[~temp_df[idempotent_key].isin(existing_ids)]\n",
    "        \n",
    "        # Insert new data into the database table\n",
    "        if not new_data.empty:\n",
    "            new_data.to_sql(table_name, engine, if_exists='append', index=False, schema=database_schema)\n",
    "            print(f\"New data appended to '{table_name}'.\")\n",
    "        else:\n",
    "            print(f\"No new data found.\")\n",
    "            \n",
    "        # Remove the temporary file after the data is inserted\n",
    "        if os.path.exists(temp_csv_path):\n",
    "            os.remove(temp_csv_path)  # Delete the temporary CSV file\n",
    "            print(f\"Temporary file '{temp_csv_path}' has been deleted.\")\n",
    "\n",
    "## Get and print the row counts after loading data\n",
    "with engine.connect() as connection:\n",
    "    current_count = connection.execute(text(f\"SELECT COUNT(*) FROM {database_schema}.{table_name}\")).scalar()\n",
    "    print(f\"The table '{table_name}' now has {current_count} rows after the update.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tsheltervenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
